{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LanguageTranslationEngDeu.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQc/OAexxbUn76VS6SrB81",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanyaS121/English-DeutschTranslator/blob/main/LanguageTranslationEngDeu.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OROPJ06_C9l3"
      },
      "source": [
        "import string\n",
        "import re\n",
        "from numpy import array, argmax, random, take\n",
        "import pandas as pd\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Embedding, Bidirectional, RepeatVector, TimeDistributed\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import load_model\n",
        "from keras import optimizers\n",
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDy-Z6z7bUv6"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jNQqCsP_dXTn"
      },
      "source": [
        "nltk.download(\"all\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c_hxQKYEJ_ov"
      },
      "source": [
        "#####Reading raw data from text file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u4YGsBYkDFVN"
      },
      "source": [
        "def read_text(filename):\n",
        "  file = open(filename, mode='rt', encoding='utf-8')\n",
        "  text = file.read()\n",
        "  file.close()\n",
        "  return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRm-tRWYJyfr"
      },
      "source": [
        "#####Splitting text into parts/sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZZvn25_DgXL"
      },
      "source": [
        "def to_lines(text):\n",
        "  senten = text.strip().split('\\n')\n",
        "  senten = [i.split('\\t') for i in senten]\n",
        "  return senten"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jr-jiqsHKwSG"
      },
      "source": [
        "#####Importing the data from the actual text file deu"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4lTYcY4oHWri"
      },
      "source": [
        "data = read_text(\"/content/sample_data/deu.txt\")\n",
        "deu_eng = to_lines(data)\n",
        "deu_eng = array(deu_eng)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIpXa_RuKIUg"
      },
      "source": [
        "#####Training only 20000 pair of sentences so to reduce the time for training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXeIsTLbJgTw"
      },
      "source": [
        "deu_eng  = deu_eng[:20000,:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSk47dhJKckP"
      },
      "source": [
        "#deu_eng"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llUOtQcmLF3v"
      },
      "source": [
        "###Text to sequence conversion###\n",
        "#####Feeding our data in Seq2Seq model\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cKu_y3BpLhI1"
      },
      "source": [
        "eng_list = []\n",
        "deu_list = []\n",
        "\n",
        "for i in deu_eng[:,0]:\n",
        "  eng_list.append(len(i.split()))\n",
        "\n",
        "for i in deu_eng[:,1]:\n",
        "  deu_list.append(len(i.split()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoQi_wyIMT2J"
      },
      "source": [
        "length_df = pd.DataFrame({'eng': eng_list, 'deu': deu_list})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5dQl-2j3Mr-O"
      },
      "source": [
        "length_df.hist(bins = 30)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nByN6O8-STFK"
      },
      "source": [
        "#####Building of a tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6AErBMXSX4S"
      },
      "source": [
        " def tokenizer(lines):\n",
        "   tokenizer = Tokenizer()\n",
        "   tokenizer.fit_on_texts(lines)\n",
        "   return tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0BOBAHJKUuW8"
      },
      "source": [
        "#####English tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jys379hU0TC"
      },
      "source": [
        "eng_tokenizer = tokenization(deu_eng[:,0])\n",
        "eng_vocab_size = len(eng_token.word_index)+1\n",
        "eng_length = 8\n",
        "print('English Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZAoc_nBbs27"
      },
      "source": [
        "#####German Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "015NRMkIbw21"
      },
      "source": [
        "deu_tokenizer = tokenization(deu_eng[:,1])\n",
        "deu_vocab_size = len(deu_tokenizer.word_index) + 1\n",
        "deu_length = 8\n",
        "print('Deutsch Vocabulary Size: %d' % vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o7bpfJrfWA2"
      },
      "source": [
        "def encode_sequences(tokenizer, length, lines):\n",
        "  seq = tokenizer.texts_to_sequences(lines)\n",
        "  seq = pad_sequences(seq, maxlen = length, padding = 'post')\n",
        "  return seq"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bpAz7bNf6yD"
      },
      "source": [
        "#Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LuoFGg8cf_8C"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test =  train_test_split(deu_eng, test_size = 0.2, random_state = 12)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0tAhTvZgqjm"
      },
      "source": [
        "#####Training Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtGVXM21guEs"
      },
      "source": [
        "trainX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "trainY = encode_sequences(eng_tokenizer, eng_length, train[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Te8A04lAhQs9"
      },
      "source": [
        "#####Test Set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJT_3VyEhTKL"
      },
      "source": [
        "testX = encode_sequences(deu_tokenizer, deu_length, train[:, 1])\n",
        "testY = encode_sequences(deu_tokenizer, deu_length, train[:, 0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Gk4dxkahmCo"
      },
      "source": [
        "#####Embedding and LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XLk88VUhpue"
      },
      "source": [
        "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
        "  model.add(LSTM(units))\n",
        "  model.add(RepeatVector(out_timesteps))\n",
        "  model.add(LSTM(units,return_sequences=True))\n",
        "  model.add(Dense(out_vocab, activation= 'softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQaeTIe_B8zC"
      },
      "source": [
        "#####Use of RMSprop optimizer is a good choice for recurrent NN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUPePEfwAm1U"
      },
      "source": [
        "model = build_model(deu_vocab_size, eng_vocab_size, deu_length, eng_length, 512)\n",
        "rms = optimizers.RMSprop(lr=0.001)\n",
        "model.compile(optimizers=rms, loss='sparse_categorial_crossentropy')\n",
        "\n",
        "#sparse_categorial_crossentropry is used as the loss function because it allows \n",
        "#us to use the the target sequence as it is instead of one hot encoded format.\n",
        "\n",
        "\n",
        "#One hot encoding might cosume the entire system's memory. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PmEH7OciCHkU"
      },
      "source": [
        "filename = 'ENG_GER_Translation'\n",
        "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
        "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1)), epochs=5, batch_size=512, validation_split =0.2, callbacks = [checkpoint], verbose=1)\n",
        "\n",
        "#increasing epochs' value might result into more accurate predictions"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_YUmmgIyIA3E"
      },
      "source": [
        "#####Training loss and validation loss"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7_sOUiwHL4J"
      },
      "source": [
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.legend(['train','validation'])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqsAQ32CIfYL"
      },
      "source": [
        "#####Loading model to make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QRwDPBkIe_Y"
      },
      "source": [
        "model = load_model('ENG_GER_Translation')\n",
        "preds = model.predict_classes(testX.reshape((testX.shape[0], testX.shape[1])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q28hfEUYI6iV"
      },
      "source": [
        "def get_word(n,tokenizer):\n",
        "  for word, index in tokenizer.word_index.items():\n",
        "    if index == n:\n",
        "      return word\n",
        "  return None"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tsci3kL9L5R_"
      },
      "source": [
        "#####Converting prediction into text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgBCHGJTL3o5"
      },
      "source": [
        "preds_text = []\n",
        "for i in preds:\n",
        "  temp = []\n",
        "  for j in range(len(i)):\n",
        "    t = get_word(i[j], eng_tokenizer)\n",
        "    if j>0:\n",
        "      if (t == get_word(i[j-1], eng_tokenizer)) or (t == None):\n",
        "        temp.append('')\n",
        "      else :\n",
        "        temp.append(t)\n",
        "    \n",
        "    else:\n",
        "      if (t == None):\n",
        "        temp.append('')\n",
        "      else:\n",
        "        temp.append(t)\n",
        "  preds_text.append(' '.join(temp))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOTM4IoYNVjp"
      },
      "source": [
        "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "slxC9mu2NigR"
      },
      "source": [
        "pd.set_option('display.max_colwidth', 200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRa72mb1NpDz"
      },
      "source": [
        "pred_df.head(15) #considering first 15 rows of the dataset"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}